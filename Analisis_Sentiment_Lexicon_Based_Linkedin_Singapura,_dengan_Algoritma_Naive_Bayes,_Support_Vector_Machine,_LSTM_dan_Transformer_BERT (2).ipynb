{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis Sentiment Lexicon Based Linkedin Singapura, dengan Algoritma Naive Bayes, Support Vector Machine, LSTM"
      ],
      "metadata": {
        "id": "W8ZmElGksiQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection"
      ],
      "metadata": {
        "id": "MlDuj2XOs0H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "metadata": {
        "id": "9Wm34Opgs3ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('linkedin_reviews_singapore_limited.csv')\n",
        "df.head(100)"
      ],
      "metadata": {
        "id": "XQAQIjZqs7Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "Wr_qOfBQuCM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "AM2tWWpPvOog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['content', 'score']]\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "_QDu4VzFuFMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "CUMs3P7suHT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "a5vla7auuuGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().any()"
      ],
      "metadata": {
        "id": "jUuqT4dIuzof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop null\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "3PaBLVexu5Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "m_D-YqQ2u-ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "5BPZRei0vC2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Pre-Processing"
      ],
      "metadata": {
        "id": "6AR_lYojvULH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Remove punctuation: Proses penghapusan simbol yang tidak relevan, nomor, tagar, dan tanda baca.\n",
        "# 2. Case folding: Proses yang membuat huruf besar menjadi kecil sehingga tidak ada kesalahan\n",
        "# mencocokan karakter atau huruf dalam kata-kata.\n",
        "# 3. Stopword removal: Proses membuang kata-kata yang tidak berpengaru\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  text = re.sub(r'\\d+', '', text)\n",
        "  text = re.sub(r'#', '', text)\n",
        "  return text\n",
        "\n",
        "def case_folding(text):\n",
        "  return text.lower()\n",
        "\n",
        "\n",
        "df['content'] = df['content'].apply(remove_punctuation)\n",
        "df['content'] = df['content'].apply(case_folding)\n",
        "\n",
        "df.head(10)\n"
      ],
      "metadata": {
        "id": "t5857sDfvGQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stopword removal\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
        "\n",
        "\n",
        "df['content'] = df['content'].apply(remove_stopwords)\n",
        "df.head(10)\n"
      ],
      "metadata": {
        "id": "TLsi-c1rwuzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove emoticon and install library emoticon\n",
        "\n",
        "def remove_emojis(text):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "  return emoji_pattern.sub(r'', text)\n",
        "\n",
        "df['content'] = df['content'].apply(remove_emojis)\n",
        "\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "HQ0C4fR_vg7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_text(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "df['content'] = df['content'].apply(tokenize_text)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "1N3UKxG1wFeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_text(text):\n",
        "  return [stemmer.stem(word) for word in text]\n",
        "\n",
        "df['content'] = df['content'].apply(stem_text)\n",
        "df.head(100)"
      ],
      "metadata": {
        "id": "0q3VaZf1xim9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labeling"
      ],
      "metadata": {
        "id": "ZIOxgktex3yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# labeling lexicon vader sentiment\n",
        "\n",
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "id": "MtRw7s-5x5uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_vader_sentiment(text):\n",
        "  scores = analyzer.polarity_scores(\" \".join(text))\n",
        "  if scores['compound'] >= 0.05:\n",
        "    return 'Positive'\n",
        "  elif scores['compound'] <= -0.05:\n",
        "    return 'Negative'\n",
        "  else:\n",
        "    return 'Neutral'\n",
        "\n",
        "df['sentiment'] = df['content'].apply(get_vader_sentiment)\n",
        "\n",
        "df.head(100)"
      ],
      "metadata": {
        "id": "uFTRL5NwyF78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot distribution of sentiment analysis\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "sentiment_counts = df['sentiment'].value_counts()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)\n",
        "plt.title('Distribution of Sentiment Analysis')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Eh08IqJLyP8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wordcloud\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Combine all the text into a single string\n",
        "all_words = ' '.join([' '.join(text) for text in df['content']])\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wlx-i6RRyfWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# naive bayes\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Join the stemmed words back into a string\n",
        "df['content_str'] = df['content'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['content_str'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = nb_classifier.predict(X_test_vec)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "90Iratpy5cm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# support vector machines\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train a Support Vector Machine (SVM) classifier\n",
        "svm_classifier = SVC(kernel='linear')  # You can experiment with different kernels\n",
        "svm_classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_svm = svm_classifier.predict(X_test_vec)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"SVM Accuracy: {accuracy_svm:.2f}\")\n",
        "\n",
        "print(\"\\nSVM Classification Report:\\n\", classification_report(y_test, y_pred_svm))\n",
        "\n",
        "print(\"\\nSVM Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n"
      ],
      "metadata": {
        "id": "VMzckmTL6M1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lstm\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=5000)  # You can adjust the number of words\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 100  # You can adjust the sequence length\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(5000, 128, input_length=max_sequence_length))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))  # 3 output classes (Positive, Negative, Neutral)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_padded, y_train_encoded, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_padded, y_test_encoded)\n",
        "print(f\"LSTM Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lstm_encoded = np.argmax(model.predict(X_test_padded), axis=-1)\n",
        "y_pred_lstm = label_encoder.inverse_transform(y_pred_lstm_encoded)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(\"\\nLSTM Classification Report:\\n\", classification_report(y_test, y_pred_lstm))\n",
        "print(\"\\nLSTM Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lstm))\n"
      ],
      "metadata": {
        "id": "czJuilry6RWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisasi Model dan Kesimpulan"
      ],
      "metadata": {
        "id": "F2nTljvA6j6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisasi Setiap Model dan Kesimpulan\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Visualisasi Model dan Kesimpulan\n",
        "\n",
        "# Naive Bayes\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Naive Bayes Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# SVM\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('SVM Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# LSTM\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_lstm), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('LSTM Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Kesimpulan\n",
        "print(\"Kesimpulan:\")\n",
        "print(\"Model Naive Bayes memiliki akurasi sebesar:\", accuracy)\n",
        "print(\"Model SVM memiliki akurasi sebesar:\", accuracy_svm)\n",
        "print(\"Model LSTM memiliki akurasi sebesar:\", accuracy)"
      ],
      "metadata": {
        "id": "vKJUqukM6p-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get requirement txt\n",
        "\n",
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "dh6VPog-_8dC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}